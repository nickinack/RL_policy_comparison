{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the study\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study is aimed to compare decay rates for a Policy map in an RL framework. Here , we try to compare the decay rates for the following methods of implementing Reinforcement Learning:\n",
    "\n",
    "      1. Iterative Reinforcement Learning\n",
    "      2. Monte Carlo Reinforcement Learning\n",
    "      \n",
    "The discount rates, gamma for each of these implementations are the same. We give equal importance to all the states, and hence the discount factor is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem goes as follows:\n",
    "\n",
    "Let us say we have a robot and we have a checkboard with numbered from (0,0) to (3,3). The checkboard had 4x4 = 16 boxes. We have a robot which starts at any random position except for the first and the last box, i.e the (0,0) coordinate and the (3,3) coordinate. We are required to direct the robot to (0,0) coordinate or the first box, or the (3,3) coordinate or the last box ; whichever one is closer. We use a Machine learning Algorithm known as the Reinforcement learning algorithm in order to find the most optimal way of directing the robot to the optimal destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirement of a Machine Learning algo in order to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may sound silly to solve this problem using Machine Learning, when the checkboard size becomes large, say, 100x100, a naive algorithm would not work considering heavy penalties for each wrong move the robot makes. Hence, we deduce a general algorithm in order solve such conditions and direct the robot in an optimal way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations' Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative reinforcement Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"rl_images/iterative.PNG\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo reinforceent Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"montecarlo.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis and effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wall timing for each of the methods are as shown below:\n",
    "     \n",
    "     1. Iterative :   2.77 s\n",
    "     2. Monte-Carlo : 11.9 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the convergence of decay graphs for gamma = 1 is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative RL decay plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'rl_images/iterative_graph.png' width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo RL decay plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='rl_images/montecarlo_graph.png' width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the values in the policy-map of Montecarlo dosen't have accurate symmetrical structures. But there exists, perfect biforcative symmetry in the Iterative-RL's policy map matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo RL Policy Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='rl_images/mcpolicy.png' width =\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative RL policy Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='rl_images/ipolicy.png' width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental results show that the Iterative RL implementation is a more effective method of solving such RL problems in comparison to Mone Carlo RL implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
